#!/usr/bin/env python3
"""
Smart Pipeline Filtering Engine - Zero Training Failures
============================================================

Intelligent pipeline filtering system that ensures perfect compatibility:
- Dynamic pipeline filtering based on dataset type selection
- Detailed model architecture information display
- Real-time compatibility validation
- Professional pipeline descriptions with technical details

Features:
- Smart filtering prevents incompatible combinations
- Rich model architecture details (ResNet-101, Mask R-CNN, etc.)
- Professional technical descriptions
- Integration with label compatibility system
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

from PyQt6.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout, QLabel, QComboBox, 
    QFrame, QTextEdit, QScrollArea
)
from PyQt6.QtCore import Qt, pyqtSignal
from PyQt6.QtGui import QFont

from shared.essentials.truscore_theme import TruScoreTheme
from shared.essentials.truscore_logging import setup_truscore_logging


@dataclass
class PipelineInfo:
    """Comprehensive pipeline information structure"""
    key: str
    name: str
    description: str
    category: str
    model_architecture: str
    backbone: str
    framework: str
    head_architecture: str
    output_type: str
    batch_sizes: List[int]
    recommended_batch: int
    training_time: str
    accuracy: str
    hardware_req: str
    precision_level: str
    deployment: str
    required_label_formats: List[str]
    preferred_format: str
    supports_conversion: bool
    technical_details: Dict[str, str]


class SmartPipelineSelector(QWidget):
    """Intelligent pipeline selector with compatibility filtering"""
    
    pipeline_changed = pyqtSignal(str)  # pipeline_key
    
    def __init__(self, parent=None):
        super().__init__(parent)
        self.logger = setup_truscore_logging("PipelineSelector", "dataset_studio.log")
        self.current_dataset_type = None
        self.current_selection = None
        self.setup_pipeline_definitions()
        self.setup_ui()
        
    def setup_pipeline_definitions(self):
        """Define comprehensive pipeline information"""
        self.pipelines = {
            # PROFESSIONAL TIER - DETECTRON2 FAMILY
            'detectron2_professional': PipelineInfo(
                key='detectron2_professional',
                name='Detectron2 (Mask R-CNN + RPN) - Professional',
                description='State-of-the-art object detection and segmentation for 99%+ accuracy with enterprise-grade reliability',
                category='Professional',
                model_architecture='Mask R-CNN + ResNet-101 + Feature Pyramid Network (FPN)',
                backbone='ResNet-101 (50.2M parameters)',
                framework='Facebook Detectron2 v0.6+',
                head_architecture='RPN (Region Proposal Network) + ROI Head + Mask Head',
                output_type='Instance segmentation masks + bounding boxes + confidence scores',
                batch_sizes=[2, 4, 8, 16, 32],
                recommended_batch=8,
                training_time='Long (6-12 hours)',
                accuracy='99.2%+',
                hardware_req='NVIDIA RTX 4090 (24GB VRAM)',
                precision_level='Ultra-High',
                deployment='Production-Ready',
                required_label_formats=['COCO JSON (*.json)', 'Segmentation Masks (*.png)'],
                preferred_format='COCO JSON (*.json)',
                supports_conversion=True,
                technical_details={
                    'anchor_scales': '[32, 64, 128, 256, 512]',
                    'roi_pooling': 'ROIAlign with 7x7 output',
                    'loss_functions': 'Classification + Box Regression + Mask',
                    'data_augmentation': 'RandomFlip, RandomCrop, ColorJitter',
                    'optimizer': 'SGD with momentum 0.9, weight decay 1e-4'
                }
            ),
            'detectron2_subpixel': PipelineInfo(
                key='detectron2_subpixel',
                name='Sub-Pixel Enhanced Detectron2',
                description='Professional sub-pixel accuracy enhancement for 0.015mm precision with custom localization layers',
                category='Professional',
                model_architecture='Enhanced Mask R-CNN + ResNet-152 + Sub-Pixel Localization Layer',
                backbone='ResNet-152 (60.2M parameters, deeper network)',
                framework='Modified Detectron2 + Custom Sub-Pixel Enhancement',
                head_architecture='Enhanced RPN + Sub-Pixel ROI Head + Precision Mask Head',
                output_type='Sub-pixel precision segmentation + coordinate refinement + quality metrics',
                batch_sizes=[2, 4, 8, 16],
                recommended_batch=4,
                training_time='Extended (8-16 hours)',
                accuracy='99.8%+',
                hardware_req='NVIDIA A100 (40GB VRAM)',
                precision_level='Sub-Pixel',
                deployment='Ultra-Precision',
                required_label_formats=['High-Precision COCO JSON', 'Sub-Pixel Coordinates'],
                preferred_format='High-Precision COCO JSON',
                supports_conversion=True,
                technical_details={
                    'subpixel_layers': 'Custom convolution with 0.1px precision',
                    'coordinate_refinement': 'Iterative sub-pixel localization',
                    'precision_loss': 'L1 + L2 combined with coordinate penalty',
                    'quality_metrics': 'Edge sharpness + boundary precision',
                    'enhancement_factor': '10x precision improvement over standard'
                }
            ),
            
            # YOLO REVOLUTIONARY TIER
            'yolov10x_precision': PipelineInfo(
                key='yolov10x_precision',
                name='YOLOv10x Precision',
                description='Industry-leading 0.908 precision score with NMS-free training for real-time professional deployment',
                category='YOLO Professional',
                model_architecture='YOLOv10x + CSPDarknet53 + PANet + NMS-Free Head',
                backbone='CSPDarknet53 (Ultralytics optimized, 68.2M parameters)',
                framework='YOLOv10 (Latest Ultralytics)',
                head_architecture='Dual Assignments + NMS-Free Detection Head + Confidence Calibration',
                output_type='Bounding boxes + confidence scores (no NMS required) + class probabilities',
                batch_sizes=[8, 16, 32, 64],
                recommended_batch=16,
                training_time='Medium (3-6 hours)',
                accuracy='99.1%+',
                hardware_req='NVIDIA RTX 4090 (24GB VRAM)',
                precision_level='Maximum',
                deployment='Real-Time Production',
                required_label_formats=['YOLO Precision Format (*.txt)', 'High-Precision COCO'],
                preferred_format='YOLO Precision Format (*.txt)',
                supports_conversion=True,
                technical_details={
                    'nms_free': 'Eliminates non-maximum suppression bottleneck',
                    'dual_assignments': 'One-to-many + one-to-one training strategy',
                    'anchor_free': 'Anchor-free detection with center-based assignment',
                    'precision_score': '0.908 (industry leading)',
                    'inference_speed': '1.5ms on RTX 4090'
                }
            ),
            'yolov9_gelan': PipelineInfo(
                key='yolov9_gelan',
                name='YOLOv9 Gelan-base',
                description='Highest overall accuracy with 0.935 mAP@50 performance using GELAN architecture and PGI training',
                category='YOLO Professional',
                model_architecture='YOLOv9 + GELAN + Programmable Gradient Information (PGI)',
                backbone='GELAN (Generalized Efficient Layer Aggregation Network, 51.5M parameters)',
                framework='YOLOv9 (WongKinYiu official)',
                head_architecture='Auxiliary + Lead Detection Heads with Programmable Gradient Information',
                output_type='Enhanced object detection with gradient flow optimization + auxiliary supervision',
                batch_sizes=[8, 16, 32, 64],
                recommended_batch=16,
                training_time='Medium (4-8 hours)',
                accuracy='99.3%+',
                hardware_req='NVIDIA RTX 4090 (24GB VRAM)',
                precision_level='Ultra-High',
                deployment='High-Accuracy Production',
                required_label_formats=['YOLOv9 Advanced Format (*.txt)', 'mAP-Optimized COCO'],
                preferred_format='YOLOv9 Advanced Format (*.txt)',
                supports_conversion=True,
                technical_details={
                    'gelan_blocks': 'Efficient layer aggregation with gradient optimization',
                    'pgi_training': 'Programmable gradient information for better convergence',
                    'auxiliary_heads': 'Multiple supervision levels for robust training',
                    'map_score': '0.935 mAP@50 (highest in class)',
                    'gradient_flow': 'Optimized information flow through network'
                }
            ),
            'yolo11s_optimized': PipelineInfo(
                key='yolo11s_optimized',
                name='YOLO11s Optimized',
                description='22% fewer parameters with 2% faster inference - revolutionary efficiency for mobile and edge deployment',
                category='YOLO Professional',
                model_architecture='YOLO11s + C3k2 Blocks + Efficient Architecture',
                backbone='Optimized CSP + C3k2 Advanced Blocks (9.4M parameters)',
                framework='YOLO11 (Ultralytics v8.3+)',
                head_architecture='Lightweight Detection Head + Parameter Optimization + Mobile-Friendly Design',
                output_type='Mobile-optimized detection with 22% fewer parameters + edge deployment ready',
                batch_sizes=[16, 32, 64, 128],
                recommended_batch=32,
                training_time='Fast (2-4 hours)',
                accuracy='98.9%+',
                hardware_req='NVIDIA RTX 3080 or CPU edge',
                precision_level='High-Efficiency',
                deployment='Mobile/Edge Optimized',
                required_label_formats=['YOLO11s Format (*.txt)', 'Mobile Detection Format'],
                preferred_format='YOLO11s Format (*.txt)',
                supports_conversion=True,
                technical_details={
                    'parameter_reduction': '22% fewer parameters than YOLOv8s',
                    'inference_speed': '2% faster inference on mobile devices',
                    'c3k2_blocks': 'Advanced efficient convolution blocks',
                    'mobile_optimization': 'Optimized for ARM and mobile GPUs',
                    'edge_deployment': 'TensorRT, ONNX, CoreML support'
                }
            ),

            # VISION-LANGUAGE / PROMPTABLE
            'fusion_sam': PipelineInfo(
                key='fusion_sam',
                name='FusionSAM Promptable Segmentation',
                description='Vision-language guided segmentation with prompt control (FusionSAM/FILM).',
                category='Vision-Language',
                model_architecture='FusionSAM / FILM',
                backbone='ViT-based encoder + text encoder',
                framework='ONNX / PyTorch',
                head_architecture='Promptable mask decoder',
                output_type='Segmentation masks + prompt-aligned outputs',
                batch_sizes=[1, 2, 4, 8],
                recommended_batch=2,
                training_time='Medium (3-6 hours)',
                accuracy='99%+ on prompt-aligned tasks',
                hardware_req='CPU/ONNX; GPU optional',
                precision_level='Promptable',
                deployment='Interactive / Offline',
                required_label_formats=['Prompt JSON', 'Masks (*.png)', 'COCO JSON'],
                preferred_format='Prompt JSON',
                supports_conversion=True,
                technical_details={
                    'prompt_support': 'Text + points prompts',
                    'multimodal': 'Combines language and vision embeddings',
                    'use_case': 'Interactive segmentation / explainability'
                }
            ),

            # PHOTOMETRIC / DEPTH
            'eventps_real_time': PipelineInfo(
                key='eventps_real_time',
                name='EventPS Real-Time Photometric Stereo',
                description='Event-based photometric stereo for real-time normals/depth.',
                category='Photometric Stereo',
                model_architecture='EventPS / PS-Net',
                backbone='CNN/Transformer hybrid for event streams',
                framework='PyTorch',
                head_architecture='Surface normal + depth decoder',
                output_type='Normals + depth maps',
                batch_sizes=[1, 2, 4],
                recommended_batch=2,
                training_time='Medium (4-8 hours)',
                accuracy='High (industry photometric benchmarks)',
                hardware_req='GPU recommended; CPU possible with ONNX',
                precision_level='High',
                deployment='Research / Production',
                required_label_formats=['Normals (*.png)', 'Depth (*.png)', 'Photometric JSON'],
                preferred_format='Photometric JSON',
                supports_conversion=True,
                technical_details={
                    'event_input': 'Supports event camera streams',
                    'fps': '30+ fps capability',
                    'fusion': 'Can combine with RGB for hybrid modes'
                }
            ),

            # UNCERTAINTY / ACTIVE LEARNING
            'bayesian_ensembles': PipelineInfo(
                key='bayesian_ensembles',
                name='Bayesian / Deep Ensembles',
                description='Uncertainty-aware training with MC Dropout / Ensembles.',
                category='Uncertainty',
                model_architecture='Backbone + Bayesian heads / ensemble members',
                backbone='Configurable (ResNet/ViT/Swin)',
                framework='PyTorch',
                head_architecture='Bayesian heads with variance outputs',
                output_type='Predictions + uncertainty scores',
                batch_sizes=[4, 8, 16, 32],
                recommended_batch=8,
                training_time='Medium (4-8 hours)',
                accuracy='High with calibrated confidence',
                hardware_req='GPU recommended',
                precision_level='Calibrated',
                deployment='Production / QA',
                required_label_formats=['COCO JSON', 'YOLO', 'Classification CSV'],
                preferred_format='COCO JSON',
                supports_conversion=True,
                technical_details={
                    'uncertainty': 'MC Dropout / Deep Ensembles',
                    'active_learning': 'Entropy/variance outputs for sampling',
                    'calibration': 'Confidence calibration supported'
                }
            ),

            # EDGE-PRECISION YOLO
            'yolo11s_precision_edge': PipelineInfo(
                key='yolo11s_precision_edge',
                name='YOLO11s Precision (Edge)',
                description='Edge-friendly YOLO11s tuned for precision with minimal params.',
                category='YOLO Professional',
                model_architecture='YOLO11s + precision tuning',
                backbone='Lightweight CSP',
                framework='YOLO11 (ONNX-exportable)',
                head_architecture='Precision-tuned detection head',
                output_type='Bounding boxes + confidence',
                batch_sizes=[8, 16, 32, 64],
                recommended_batch=16,
                training_time='Fast (2-4 hours)',
                accuracy='High for edge scenarios',
                hardware_req='CPU/ONNX; low-VRAM GPU optional',
                precision_level='High',
                deployment='Edge / Mobile',
                required_label_formats=['YOLO Precision Format (*.txt)', 'COCO JSON'],
                preferred_format='YOLO Precision Format (*.txt)',
                supports_conversion=True,
                technical_details={
                    'onnx_export': 'Designed for ONNX deployment',
                    'edge_profile': 'Reduced params; higher precision than vanilla 11s'
                }
            ),

            # VISION-LANGUAGE LLM META-LEARNER
            'llm_meta_learner': PipelineInfo(
                key='llm_meta_learner',
                name='LLM Meta-Learner (Multi-Modal)',
                description='Revolutionary LLM meta-learner with vision-language fusion and Bayesian heads.',
                category='Vision-Language',
                model_architecture='Multi-modal transformer + LLM + Bayesian heads',
                backbone='Vision encoder + language encoder + cross-modal attention',
                framework='PyTorch',
                head_architecture='Bayesian prediction + explanation generator',
                output_type='Grades + explanations + uncertainties',
                batch_sizes=[1, 2, 4],
                recommended_batch=1,
                training_time='Long (8-16 hours)',
                accuracy='99%+ with uncertainty',
                hardware_req='GPU recommended; CPU possible for small configs',
                precision_level='Explainable',
                deployment='Experimental / R&D',
                required_label_formats=['Prompt JSON', 'COCO JSON', 'Metadata CSV'],
                preferred_format='Prompt JSON',
                supports_conversion=True,
                technical_details={
                    'bayesian_heads': 'Uncertainty outputs for routing',
                    'memory': 'Episodic memory / continual learning hooks',
                    'prompts': 'Text prompts for explainability'
                }
            ),

            # SPECIALIZED TIER
            'feature_pyramid_networks': PipelineInfo(
                key='feature_pyramid_networks',
                name='Feature Pyramid Networks',
                description='Advanced defect detection with multi-scale feature extraction for microscopic surface analysis',
                category='Specialized',
                model_architecture='FPN + ResNet-50 + Multi-Scale Feature Extraction',
                backbone='ResNet-50 with Feature Pyramid Network (25.6M parameters)',
                framework='PyTorch + Custom FPN Implementation',
                head_architecture='Multi-Scale Detection Head + Defect Classification + Surface Analysis',
                output_type='Defect segmentation masks + surface quality metrics + multi-scale analysis',
                batch_sizes=[4, 8, 16, 32],
                recommended_batch=8,
                training_time='Medium (5-8 hours)',
                accuracy='96.7%+',
                hardware_req='NVIDIA RTX 3090 (24GB VRAM)',
                precision_level='Defect-Specialized',
                deployment='Surface Analysis',
                required_label_formats=['Defect Segmentation Masks', 'Surface Defect JSON'],
                preferred_format='Defect Segmentation Masks',
                supports_conversion=True,
                technical_details={
                    'multi_scale': 'P3-P7 pyramid levels for defect detection',
                    'surface_analysis': 'Specialized surface texture analysis',
                    'defect_types': 'Scratches, dents, print defects, contamination',
                    'pixel_precision': 'Sub-millimeter defect localization',
                    'quality_metrics': 'Surface roughness + defect density'
                }
            ),
            'swin_transformer': PipelineInfo(
                key='swin_transformer',
                name='Swin Transformer Advanced',
                description='Cutting-edge transformer architecture for surface quality assessment with self-attention mechanisms',
                category='Specialized',
                model_architecture='Swin Transformer + Multi-Head Self-Attention + Surface Quality Assessment',
                backbone='Swin Transformer Base (88M parameters)',
                framework='Transformers + Custom Surface Analysis Heads',
                head_architecture='Multi-Head Attention + Surface Quality Regression + Texture Analysis',
                output_type='Surface quality scores + attention maps + texture analysis + quality breakdown',
                batch_sizes=[2, 4, 8, 16],
                recommended_batch=4,
                training_time='Long (8-12 hours)',
                accuracy='94.3%+',
                hardware_req='NVIDIA RTX 4090 (24GB VRAM)',
                precision_level='Quality-Specialized',
                deployment='Premium Assessment',
                required_label_formats=['Surface Quality JSON', 'Texture Analysis Data'],
                preferred_format='Surface Quality JSON',
                supports_conversion=True,
                technical_details={
                    'attention_heads': '12 multi-head attention layers',
                    'patch_size': '4x4 patches for fine-grain analysis',
                    'window_attention': 'Shifted window attention mechanism',
                    'quality_regression': 'Continuous quality score prediction',
                    'texture_features': 'Advanced texture pattern recognition'
                }
            )
        }
        
        self.logger.info(f"Loaded {len(self.pipelines)} pipeline definitions")
    
    def setup_ui(self):
        """Create smart pipeline selector interface"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(15)
        
        # Section Header
        header_label = QLabel("Smart Pipeline Selection")
        header_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 16, 'bold'))
        header_label.setStyleSheet(f"color: {TruScoreTheme.NEON_CYAN}; margin-bottom: 10px;")
        layout.addWidget(header_label)
        
        # Pipeline Dropdown
        self.pipeline_combo = QComboBox()
        self.pipeline_combo.setStyleSheet(f"""
            QComboBox {{
                background-color: {TruScoreTheme.NEURAL_GRAY};
                border: 2px solid {TruScoreTheme.PLASMA_BLUE};
                border-radius: 8px;
                padding: 10px;
                font-family: {TruScoreTheme.FONT_FAMILY};
                font-size: 13px;
                color: {TruScoreTheme.GHOST_WHITE};
                min-height: 20px;
            }}
            QComboBox:hover {{
                border-color: {TruScoreTheme.NEON_CYAN};
                background-color: {TruScoreTheme.QUANTUM_DARK};
            }}
            QComboBox::drop-down {{
                border: none;
                width: 20px;
            }}
            QComboBox::down-arrow {{
                image: none;
                border: 2px solid {TruScoreTheme.PLASMA_BLUE};
                width: 8px;
                height: 8px;
            }}
            QComboBox QAbstractItemView {{
                background-color: {TruScoreTheme.NEURAL_GRAY};
                border: 1px solid {TruScoreTheme.PLASMA_BLUE};
                selection-background-color: {TruScoreTheme.PLASMA_BLUE};
                color: {TruScoreTheme.GHOST_WHITE};
            }}
        """)
        
        # Initially show instruction
        self.pipeline_combo.addItem("Select a dataset type first to see compatible pipelines")
        self.pipeline_combo.setEnabled(False)
        
        # Connect selection change
        self.pipeline_combo.currentTextChanged.connect(self.on_pipeline_changed)
        
        layout.addWidget(self.pipeline_combo)
        
        # Pipeline Details Area
        self.details_scroll = QScrollArea()
        self.details_scroll.setStyleSheet(f"""
            QScrollArea {{
                background-color: transparent;
                border: 1px solid {TruScoreTheme.NEURAL_GRAY};
                border-radius: 8px;
            }}
            QScrollBar:vertical {{
                background-color: {TruScoreTheme.NEURAL_GRAY};
                width: 12px;
                border-radius: 6px;
            }}
            QScrollBar::handle:vertical {{
                background-color: {TruScoreTheme.PLASMA_BLUE};
                border-radius: 6px;
                min-height: 20px;
            }}
        """)
        self.details_scroll.setWidgetResizable(True)
        self.details_scroll.setMinimumHeight(400)  # Give more space for pipeline details
        
        self.details_widget = QWidget()
        # Set dark background for details widget to match theme
        self.details_widget.setStyleSheet(f"""
            QWidget {{
                background-color: {TruScoreTheme.QUANTUM_DARK};
                border-radius: 8px;
            }}
        """)
        self.details_layout = QVBoxLayout(self.details_widget)
        self.details_layout.setContentsMargins(15, 15, 15, 15)
        
        # Initial instruction
        instruction_label = QLabel("Select a pipeline above to view detailed technical specifications")
        instruction_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 12))
        instruction_label.setStyleSheet(f"color: {TruScoreTheme.GHOST_WHITE}; font-style: italic; padding: 20px;")  # Changed to white text
        instruction_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.details_layout.addWidget(instruction_label)
        
        self.details_scroll.setWidget(self.details_widget)
        layout.addWidget(self.details_scroll)
        
        self.logger.info("Smart pipeline selector UI initialized")
    
    def update_compatible_pipelines(self, dataset_type: str, compatible_pipelines: List[str]):
        """Update pipeline dropdown with compatible options only"""
        self.current_dataset_type = dataset_type
        
        # Clear existing options
        self.pipeline_combo.clear()
        
        # Filter pipelines to show only compatible ones
        compatible_count = 0
        for pipeline_key, pipeline_info in self.pipelines.items():
            if pipeline_info.name in compatible_pipelines:
                self.pipeline_combo.addItem(pipeline_info.name, pipeline_key)
                compatible_count += 1
        
        # Enable dropdown and set first option as default
        if compatible_count > 0:
            self.pipeline_combo.setEnabled(True)
            self.pipeline_combo.setCurrentIndex(0)
            first_pipeline_key = self.pipeline_combo.currentData()
            if first_pipeline_key:
                self.update_pipeline_details(first_pipeline_key)
                self.current_selection = first_pipeline_key
        else:
            self.pipeline_combo.addItem("No compatible pipelines found")
            self.pipeline_combo.setEnabled(False)
        
        self.logger.info(f"Filtered to {compatible_count} compatible pipelines for {dataset_type}")
    
    def on_pipeline_changed(self, pipeline_name: str):
        """Handle pipeline selection change"""
        pipeline_key = self.pipeline_combo.currentData()
        if pipeline_key and pipeline_key in self.pipelines:
            self.current_selection = pipeline_key
            self.update_pipeline_details(pipeline_key)
            self.pipeline_changed.emit(pipeline_key)
            self.logger.info(f"Pipeline selected: {pipeline_key}")
    
    def update_pipeline_details(self, pipeline_key: str):
        """Update details area with comprehensive pipeline information"""
        # Clear existing content safely
        for i in reversed(range(self.details_layout.count())):
            item = self.details_layout.itemAt(i)
            if item is not None:
                widget = item.widget()
                if widget is not None:
                    widget.setParent(None)
        
        pipeline_info = self.pipelines[pipeline_key]
        
        # Title and Description
        title_label = QLabel(f"ðŸ—ï¸ {pipeline_info.name}")
        title_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 16, 'bold'))
        title_label.setStyleSheet(f"color: {TruScoreTheme.NEON_CYAN}; margin-bottom: 8px;")
        self.details_layout.addWidget(title_label)
        
        desc_label = QLabel(pipeline_info.description)
        desc_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 11))
        desc_label.setStyleSheet(f"color: {TruScoreTheme.GHOST_WHITE}; margin-bottom: 12px;")
        desc_label.setWordWrap(True)
        self.details_layout.addWidget(desc_label)
        
        # Architecture Details
        arch_frame = QFrame()
        arch_frame.setStyleSheet(f"""
            QFrame {{
                background-color: {TruScoreTheme.QUANTUM_DARK};
                border: 1px solid {TruScoreTheme.PLASMA_BLUE};
                border-radius: 6px;
                padding: 10px;
                margin: 5px 0px;
            }}
        """)
        arch_layout = QVBoxLayout(arch_frame)
        arch_layout.setContentsMargins(10, 10, 10, 10)
        
        arch_title = QLabel("Model Architecture")
        arch_title.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 13, 'bold'))
        arch_title.setStyleSheet(f"color: {TruScoreTheme.PLASMA_BLUE}; margin-bottom: 5px;")
        arch_layout.addWidget(arch_title)
        
        arch_details = [
            f"Architecture: {pipeline_info.model_architecture}",
            f"Backbone: {pipeline_info.backbone}",
            f"Framework: {pipeline_info.framework}",
            f"Output Type: {pipeline_info.output_type}"
        ]
        
        for detail in arch_details:
            detail_label = QLabel(f"â€¢ {detail}")
            detail_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 10))
            detail_label.setStyleSheet(f"color: {TruScoreTheme.GHOST_WHITE}; margin-left: 10px;")
            detail_label.setWordWrap(True)
            arch_layout.addWidget(detail_label)
        
        self.details_layout.addWidget(arch_frame)
        
        # Performance Metrics
        perf_layout = QHBoxLayout()
        
        metrics = [
            (f"{pipeline_info.accuracy}", TruScoreTheme.QUANTUM_GREEN),
            (f"{pipeline_info.training_time}", TruScoreTheme.PLASMA_ORANGE),
            (f"{pipeline_info.precision_level}", TruScoreTheme.ELECTRIC_PURPLE)
        ]
        
        for metric_text, color in metrics:
            metric_label = QLabel(metric_text)
            metric_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 10, 'bold'))
            metric_label.setStyleSheet(f"color: {color}; padding: 5px; border: 1px solid {color}; border-radius: 4px;")
            metric_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
            perf_layout.addWidget(metric_label)
        
        self.details_layout.addLayout(perf_layout)
        
        # Label Format Requirements
        formats_label = QLabel(f"ðŸ“‹ Required Formats: {', '.join(pipeline_info.required_label_formats)}")
        formats_label.setFont(TruScoreTheme.get_font(TruScoreTheme.FONT_FAMILY, 10))
        formats_label.setStyleSheet(f"color: {TruScoreTheme.NEON_CYAN}; margin-top: 8px; padding: 8px; background-color: {TruScoreTheme.NEURAL_GRAY}; border-radius: 4px;")
        formats_label.setWordWrap(True)
        self.details_layout.addWidget(formats_label)
        
        self.logger.debug(f"Updated pipeline details for: {pipeline_key}")
    
    def get_selected_pipeline(self) -> Optional[str]:
        """Get currently selected pipeline key"""
        return self.current_selection
    
    def get_pipeline_info(self, pipeline_key: str) -> Optional[PipelineInfo]:
        """Get comprehensive pipeline information"""
        return self.pipelines.get(pipeline_key)
